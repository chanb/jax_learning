{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a1ad97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from IPython.core.debugger import set_trace\n",
    "from tqdm import tqdm\n",
    "from typing import List, Sequence, Tuple\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "\n",
    "from jax_learning.models.embeddings import PositionalEmbedding1D\n",
    "from jax_learning.models.layers import MLP, Conv2D, Conv2DTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8dced39",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb673f",
   "metadata": {},
   "source": [
    "# Download MNIST\n",
    "Reference: https://github.com/hsjeong5/MNIST-for-Numpy/blob/master/mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18238cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading MNIST dataset\n",
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n",
      "(6742, 784) (6742,)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "if not os.path.isfile(\"mnist.pkl\"):\n",
    "    init()\n",
    "    \n",
    "print(\"loading MNIST dataset\")\n",
    "(train_x, train_y, test_x, test_y) = load()\n",
    "train_x = train_x.astype(float) / 255\n",
    "test_x = test_x.astype(float) / 255\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "\n",
    "train_x = train_x[train_y == 1]\n",
    "train_y = train_y[train_y == 1]\n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "train_x = train_x[[0]]\n",
    "train_y = train_y[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2aca12",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741606a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def SAC_ENCODER(in_channels):\n",
    "    return (\n",
    "        [in_channels, 32, (3, 3), (2, 2), (0, 0), (1, 1), jax.nn.gelu, False],\n",
    "        [32, 32, (3, 3), (1, 1), (0, 0), (1, 1), jax.nn.gelu, False],\n",
    "        [32, 32, (3, 3), (1, 1), (0, 0), (1, 1), jax.nn.gelu, False],\n",
    "        [32, 32, (3, 3), (1, 1), (0, 0), (1, 1), jax.nn.gelu, False],\n",
    "    )\n",
    "\n",
    "def SAC_DECODER(out_channels, in_dim):\n",
    "    return (\n",
    "        [32, 32, (3, 3), (1, 1), (0, 0), (1, 1), (0, 0), jax.nn.gelu, False],\n",
    "        [32, 32, (3, 3), (1, 1), (0, 0), (1, 1), (0, 0), jax.nn.gelu, False],\n",
    "        [32, 32, (3, 3), (1, 1), (0, 0), (1, 1), (0, 0), jax.nn.gelu, False],\n",
    "        [32, out_channels, (3, 3), (2, 2), (0, 0), (1, 1), (in_dim[0] % 2 == 0, in_dim[1] % 2 == 0), identity, False],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d25485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(eqx.Module):\n",
    "    gen_dim: Sequence[int] = eqx.static_field()\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        t: float,\n",
    "    ) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "            \n",
    "    \n",
    "class MLPModel(Model):\n",
    "    model: eqx.Module\n",
    "    emb: eqx.nn.Embedding = eqx.static_field()\n",
    "    T: int = eqx.static_field()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gen_dim: Sequence[int],\n",
    "        context_dim: Sequence[int],\n",
    "        T: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_hidden: int,\n",
    "        key: jrandom.PRNGKey,\n",
    "    ):\n",
    "        gen_dim = gen_dim\n",
    "        self.emb = PositionalEmbedding1D(T, embedding_dim, key)\n",
    "        self.model = MLP(gen_dim + context_dim + embedding_dim, gen_dim, hidden_dim, num_hidden, key, activation=jax.nn.gelu)\n",
    "        self.T = T\n",
    "        super().__init__(gen_dim)\n",
    "        \n",
    "    @jax.jit\n",
    "    def predict(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        c: np.ndarray,\n",
    "        t: int,\n",
    "    ) -> np.ndarray:\n",
    "        t_emb = self.emb(t)\n",
    "        x_t = jnp.concatenate((x, c, t_emb))\n",
    "        return self.model(x_t)\n",
    "\n",
    "\n",
    "class CNNModel(Model):\n",
    "    conv2d_layers: eqx.Module\n",
    "    convtranspose2d_layers: eqx.Module\n",
    "    mlp_layers: eqx.Module\n",
    "    num_channels: int = eqx.static_field()\n",
    "    emb: eqx.nn.Embedding = eqx.static_field()\n",
    "    T: int = eqx.static_field()\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        gen_dim: Sequence[int],\n",
    "        context_dim: Sequence[int],\n",
    "        num_channels: int,\n",
    "        T: int,\n",
    "        embedding_dim: int,\n",
    "        key: jrandom.PRNGKey,\n",
    "    ):\n",
    "        self.conv2d_layers = Conv2D(SAC_ENCODER(in_channels=num_channels),\n",
    "                                    in_dim=gen_dim,\n",
    "                                    key=jrandom.PRNGKey(seed))\n",
    "        self.convtranspose2d_layers = Conv2DTranspose(SAC_DECODER(out_channels=num_channels,\n",
    "                                                                  in_dim=gen_dim),\n",
    "                                                      key=jrandom.PRNGKey(seed))\n",
    "        conv_out_dim = int(np.product(self.conv2d_layers.out_dim))\n",
    "        self.mlp_layers = MLP(conv_out_dim + context_dim + embedding_dim,\n",
    "                              conv_out_dim, 256, 4, key=jrandom.PRNGKey(seed), activation=jax.nn.gelu)\n",
    "        self.num_channels = num_channels\n",
    "        self.emb = PositionalEmbedding1D(T, embedding_dim, key)\n",
    "        self.T = T\n",
    "\n",
    "        super().__init__(gen_dim)\n",
    "        \n",
    "    @jax.jit\n",
    "    def predict(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        c: np.ndarray,\n",
    "        t: int,\n",
    "    ) -> np.ndarray:\n",
    "        z = self.conv2d_layers(x.reshape((self.num_channels, *self.gen_dim)))\n",
    "        t_emb = self.emb(t)\n",
    "        z_out = self.mlp_layers(jnp.concatenate((z.reshape(-1), c, t_emb)))\n",
    "        x_t = self.convtranspose2d_layers(z_out.reshape(self.conv2d_layers.out_dim)).reshape(-1)\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c854504",
   "metadata": {},
   "source": [
    "# DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d7b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "lo = 0.0001\n",
    "hi = 0.02\n",
    "delta = (hi - lo) / (T - 1)\n",
    "betas = jnp.array(np.arange(lo, hi + delta, delta))\n",
    "alphas = 1 - betas\n",
    "alpha_cum_prods = jnp.cumprod(alphas)\n",
    "\n",
    "sqrt_alphas = jnp.sqrt(alphas)\n",
    "sqrt_alpha_cumprods = jnp.sqrt(alpha_cum_prods)\n",
    "sqrt_one_minus_alpha_cumprods = jnp.sqrt(1 - alpha_cum_prods)\n",
    "\n",
    "# For sampling\n",
    "variances = jnp.concatenate((np.zeros(0), betas[1:] * (1 - alpha_cum_prods[:-1]) / (1 - alpha_cum_prods[1:])))\n",
    "inv_sqrt_alphas = 1 / sqrt_alphas\n",
    "noise_coef = betas / sqrt_one_minus_alpha_cumprods\n",
    "\n",
    "def add_noise(x_init: np.ndarray, noise: np.ndarray, t: np.int32):\n",
    "    return sqrt_alpha_cumprods[t, None] * x_init + sqrt_one_minus_alpha_cumprods[t, None] * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd2ceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_noises = np.random.randn(T, *train_x[[0]].shape)\n",
    "res = [add_noise(train_x[[0]], test_noises[t], t) for t in range(T)]\n",
    "fig = plt.figure(figsize=(50, 50))\n",
    "for idx, img in enumerate(res):\n",
    "    ax = fig.add_subplot(20, 10, 1 + idx)\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow(img.reshape((28, 28)))\n",
    "\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1afbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(preds: np.ndarray, targs: np.ndarray) -> np.ndarray:\n",
    "    return (preds - targs) ** 2\n",
    "\n",
    "@eqx.filter_grad(has_aux=True)\n",
    "def compute_loss(model: Model, xs: np.ndarray, cs: np.ndarray, timesteps: np.ndarray, targs: np.ndarray):\n",
    "    preds = jax.vmap(model.predict)(xs, cs, timesteps)\n",
    "    ind_loss = jax.vmap(squared_loss)(preds, targs)\n",
    "    loss = jnp.mean(ind_loss)\n",
    "    return loss, {\"loss\": loss, \"ind_loss\": ind_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model: Model, context: np.ndarray, gen_dim: Sequence[int], T: int):\n",
    "    xs = [np.random.randn(*gen_dim)]\n",
    "    for t in range(T - 1, -1 , -1):\n",
    "        var_noise = np.random.randn(*gen_dim) * int(t > 0)\n",
    "        xs.append(inv_sqrt_alphas[t] * (xs[-1] - noise_coef[t] * model.predict(xs[-1], context, t)) + variances[t] * var_noise)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_x: np.ndarray,\n",
    "    train_c: np.ndarray,\n",
    "    model: Model,\n",
    "    opt: optax.GradientTransformation,\n",
    "    cfg: Namespace\n",
    ") -> Tuple[Model, List]:\n",
    "    \"\"\"\n",
    "    train_x: data to reconstruct\n",
    "    train_c: data context\n",
    "    model: the diffusion model\n",
    "    opt: optimizer for changing the parameters of the model\n",
    "    \"\"\"\n",
    "    opt_state = opt.init(model)\n",
    "    losses = []\n",
    "    \n",
    "    try:\n",
    "        pbar = tqdm(range(cfg.num_iterations))\n",
    "        for i in pbar:\n",
    "            train_idxes = np.random.choice(len(train_x), size=cfg.batch_size)\n",
    "            curr_c = train_c[train_idxes]\n",
    "            curr_x = train_x[train_idxes]\n",
    "            timesteps = np.random.randint(cfg.T, size=cfg.batch_size)\n",
    "\n",
    "            noise = np.random.randn(*curr_x.shape)\n",
    "            noisy_x = add_noise(curr_x, noise, timesteps)\n",
    "    #         fig = plt.figure(figsize=(100, 10))\n",
    "    #         for idx, curr_noisy_x in enumerate(noisy_x):\n",
    "    #             ax = fig.add_subplot(4, cfg.batch_size // 4, 1 + idx)\n",
    "    #             ax.axis(\"off\")\n",
    "    #             plt.imshow(curr_noisy_x.reshape((28, 28)))\n",
    "    #         fig.subplots_adjust(hspace=0, wspace=0)\n",
    "    #         plt.show()\n",
    "\n",
    "            grads, info = compute_loss(model, noisy_x, curr_c[:, None], timesteps, noise)\n",
    "\n",
    "            updates, opt_state = opt.update(grads, opt_state)\n",
    "            model = eqx.apply_updates(model, updates)\n",
    "            if not np.isfinite(info[\"loss\"]):\n",
    "                assert 0\n",
    "            curr_loss = info[\"loss\"]\n",
    "            pbar.set_description(f\"loss: {curr_loss:.5f}\")\n",
    "            losses.append(info[\"loss\"])\n",
    "            if i == 0 or (i + 1) % 1000 == 0:\n",
    "                preds = sample(model, curr_c[[0]], (cfg.gen_dim,), cfg.T)\n",
    "                fig = plt.figure(figsize=(100, 10))\n",
    "                for idx, pred in enumerate(preds[1:]):\n",
    "                    ax = fig.add_subplot(4, T // 4, 1 + idx)\n",
    "                    ax.axis(\"off\")\n",
    "                    plt.imshow(pred.reshape((28, 28)))\n",
    "\n",
    "                fig.subplots_adjust(hspace=0, wspace=0)\n",
    "                plt.show()\n",
    "    except:\n",
    "        set_trace()\n",
    "        \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68615f0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen_dim = int(np.product(train_x.shape[1:]))\n",
    "context_dim = 1\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "num_hidden = 4\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "model = MLPModel(gen_dim, context_dim, T, embedding_dim, hidden_dim, num_hidden, key)\n",
    "# model = CNNModel((28, 28), 1, 1, T, embedding_dim, key)\n",
    "\n",
    "max_grad_norm = 1.\n",
    "lr = 3e-4\n",
    "opt_transforms = [optax.clip_by_global_norm(max_grad_norm), optax.scale_by_rms(), optax.scale(-lr)]\n",
    "opt = optax.chain(*opt_transforms)\n",
    "\n",
    "cfg_dict = {\n",
    "    \"num_iterations\": 1000000,\n",
    "    \"batch_size\": 256,\n",
    "    \"T\": T,\n",
    "    \"gen_dim\": gen_dim,\n",
    "}\n",
    "cfg = Namespace(**cfg_dict)\n",
    "\n",
    "trained_model, losses = train(train_x, train_y, model, opt, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd099e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(len(losses)), losses)\n",
    "ax.set_xlabel(\"Number of updates\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe676d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample(trained_model, train_y[[0]], (gen_dim,), T)\n",
    "baseline_preds = sample(model, train_y[[0]], (gen_dim,), T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ebba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50, 5))\n",
    "ax = fig.add_subplot(3, T, 1)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow((train_x[0] * data_std + data_mean).reshape((28, 28)))\n",
    "\n",
    "for idx, baseline_pred in enumerate(baseline_preds):\n",
    "    ax = fig.add_subplot(3, T + 1, T + 1 + 1 + idx)\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow((baseline_pred * data_std + data_mean).reshape((28, 28)))\n",
    "    \n",
    "for idx, pred in enumerate(preds):\n",
    "    ax = fig.add_subplot(3, T + 1, 2 * (T + 1) + 1 + idx)\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow((pred * data_std + data_mean).reshape((28, 28)))\n",
    "\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff009d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
