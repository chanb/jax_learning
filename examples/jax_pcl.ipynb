{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3ba0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/research/personal/jax_learning/.venv/lib/python3.10/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import timeit\n",
    "import wandb\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "\n",
    "from jax_learning.agents.rl_agents import RLAgent\n",
    "from jax_learning.buffers.ram_buffers import TrajectoryNumPyBuffer\n",
    "from jax_learning.common import init_wandb\n",
    "from jax_learning.constants import DISCRETE, CONTINUOUS\n",
    "from jax_learning.rl_utils import interact, evaluate, random_exploration_generator\n",
    "from jax_learning.learners.path_consistency import PCL\n",
    "from jax_learning.models import Temperature\n",
    "from jax_learning.models.policies import MLPSquashedGaussianPolicy, MLPGaussianPolicy\n",
    "from jax_learning.models.value_functions import MLPValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a46b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/research/personal/jax_learning/.venv/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n"
     ]
    }
   ],
   "source": [
    "# Can be \"online\", \"offline\" or \"disabled\".\n",
    "init_wandb(project=\"test_jax_rl\", group=\"hopper-pcl_test\", mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68d3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\n",
    "    # Environment setup\n",
    "    \"env\": \"Hopper-v2\",\n",
    "    \"seed\": 0,\n",
    "    \"render\": False,\n",
    "    \n",
    "    # Experiment progress\n",
    "    \"load_step\": 0,\n",
    "    \"log_interval\": 5000,\n",
    "    \n",
    "    # Learning hyperparameters\n",
    "    \"max_timesteps\": 1000000,\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"buffer_warmup\": 1000,\n",
    "    \"num_gradient_steps\": 1,\n",
    "    \"batch_size\": 50,\n",
    "    \"max_grad_norm\": False,\n",
    "    \"gamma\": 0.99,\n",
    "    \"update_frequency\": 1,\n",
    "    \n",
    "    \"exploration_steps\": 1000,\n",
    "    \"exploration_strategy\": \"standard_gaussian\",\n",
    "    \n",
    "    # Actor\n",
    "    \"actor_lr\": 3e-4,\n",
    "    \n",
    "    # Critic\n",
    "    \"critic_lr\": 3e-4,\n",
    "    \n",
    "    \"horizon_length\": 20,\n",
    "    \n",
    "    # Normalization\n",
    "    \"normalize_obs\": False,\n",
    "    \"normalize_value\": False,\n",
    "    \n",
    "    # Temperature\n",
    "    \"alpha_lr\": 3e-4,\n",
    "    \"init_alpha\": 0.2,\n",
    "    \"target_entropy\": None,\n",
    "    \n",
    "    # Model architecture\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_hidden\": 2,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"evaluation_frequency\": 5000,\n",
    "    \"eval_cfg\": {\n",
    "        \"num_episodes\": 10,\n",
    "        \"seed\": 1,\n",
    "        \"render\": False,\n",
    "        \"clip_action\": True,\n",
    "        \"max_action\": 1.,\n",
    "        \"min_action\": -1.,\n",
    "    }\n",
    "}\n",
    "cfg = Namespace(**cfg_dict)\n",
    "eval_cfg = Namespace(**cfg.eval_cfg)\n",
    "wandb.config = cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733f2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "502213a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/research/personal/jax_learning/.venv/lib/python3.10/site-packages/gym/envs/registration.py:592: UserWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "/Users/chanb/research/personal/jax_learning/.venv/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/research/personal/jax_learning/.venv/lib/python3.10/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/chanb/research/personal/jax_learning/.venv/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(cfg.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ed36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.obs_dim = env.observation_space.shape\n",
    "cfg.act_dim = env.action_space.shape\n",
    "if cfg.target_entropy == \"auto\":\n",
    "    cfg.target_entropy = -float(np.product(env.action_space.shape))\n",
    "cfg.action_space = CONTINUOUS\n",
    "\n",
    "cfg.random_exploration = None\n",
    "if getattr(cfg ,\"exploration_steps\", False):\n",
    "    cfg.random_exploration = random_exploration_generator(cfg.exploration_strategy,\n",
    "                                                          cfg.act_dim,\n",
    "                                                          getattr(cfg, \"min_action\", -1.),\n",
    "                                                          getattr(cfg, \"max_action\", 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.h_state_dim = (1,)\n",
    "cfg.rew_dim = (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3591254",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.buffer_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.env_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.agent_key, cfg.model_key = jrandom.split(jrandom.PRNGKey(cfg.seed), num=2)\n",
    "eval_cfg.env_rng = np.random.RandomState(eval_cfg.seed)\n",
    "cfg.evaluation_cfg = eval_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a581441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(env='Hopper-v2', seed=0, render=False, load_step=0, log_interval=5000, max_timesteps=1000000, buffer_size=1000000, buffer_warmup=1000, num_gradient_steps=1, batch_size=50, max_grad_norm=False, gamma=0.99, update_frequency=1, exploration_steps=1000, exploration_strategy='standard_gaussian', actor_lr=0.0003, critic_lr=0.0003, horizon_length=20, normalize_obs=False, normalize_value=False, alpha_lr=0.0003, init_alpha=0.2, target_entropy=None, hidden_dim=256, num_hidden=2, evaluation_frequency=5000, eval_cfg={'num_episodes': 10, 'seed': 1, 'render': False, 'clip_action': True, 'max_action': 1.0, 'min_action': -1.0}, obs_dim=(11,), act_dim=(3,), action_space='continuous', random_exploration=<function random_exploration_generator.<locals>.sample_standard_gaussian at 0x175559ea0>, h_state_dim=(1,), rew_dim=(1,), buffer_rng=RandomState(MT19937) at 0x14FD0CA40, env_rng=RandomState(MT19937) at 0x14FD0CD40, agent_key=DeviceArray([4146024105,  967050713], dtype=uint32), model_key=DeviceArray([2718843009, 1272950319], dtype=uint32), evaluation_cfg=Namespace(num_episodes=10, seed=1, render=False, clip_action=True, max_action=1.0, min_action=-1.0, env_rng=RandomState(MT19937) at 0x14FD0CC40))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e72cdf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "POLICY = \"policy\"\n",
    "Q = \"q\"\n",
    "V = \"v\"\n",
    "TEMPERATURE = \"temperature\"\n",
    "\n",
    "buffer = TrajectoryNumPyBuffer(\n",
    "    buffer_size=cfg.buffer_size,\n",
    "    obs_dim=cfg.obs_dim,\n",
    "    h_state_dim=cfg.h_state_dim,\n",
    "    act_dim=(1,) if cfg.action_space == DISCRETE else cfg.act_dim,\n",
    "    rew_dim=cfg.rew_dim,\n",
    "    rng=cfg.buffer_rng,\n",
    ")\n",
    "\n",
    "policy_key, v_key = jrandom.split(cfg.model_key)\n",
    "policy = MLPGaussianPolicy(\n",
    "        obs_dim=cfg.obs_dim,\n",
    "        act_dim=cfg.act_dim,\n",
    "        hidden_dim=cfg.hidden_dim,\n",
    "        num_hidden=cfg.num_hidden,\n",
    "        key=policy_key,\n",
    ")\n",
    "# policy = MLPSquashedGaussianPolicy(\n",
    "#         obs_dim=cfg.obs_dim,\n",
    "#         act_dim=cfg.act_dim,\n",
    "#         hidden_dim=cfg.hidden_dim,\n",
    "#         num_hidden=cfg.num_hidden,\n",
    "#         key=policy_key,\n",
    "# )\n",
    "\n",
    "temperature = Temperature(init_alpha=cfg.init_alpha)\n",
    "\n",
    "v = MLPValue(in_dim=cfg.obs_dim,\n",
    "             out_dim=(1,),\n",
    "             hidden_dim=cfg.hidden_dim,\n",
    "             num_hidden=cfg.num_hidden,\n",
    "             key=v_key,)\n",
    "\n",
    "model = {\n",
    "    POLICY: policy,\n",
    "    TEMPERATURE: temperature,\n",
    "    V: v,\n",
    "}\n",
    "\n",
    "v_opt_transforms = [\n",
    "    optax.scale_by_adam(),\n",
    "    optax.scale(-cfg.critic_lr)\n",
    "]\n",
    "\n",
    "policy_opt_transforms = [\n",
    "    optax.scale_by_adam(),\n",
    "    optax.scale(-cfg.actor_lr)\n",
    "]\n",
    "\n",
    "temperature_opt_transforms = [\n",
    "    optax.scale_by_adam(),\n",
    "    optax.scale(-cfg.alpha_lr)\n",
    "]\n",
    "\n",
    "if cfg.max_grad_norm:\n",
    "    v_opt_transforms.insert(0, optax.clip_by_global_norm(cfg.max_grad_norm))\n",
    "    policy_opt_transforms.insert(0, optax.clip_by_global_norm(cfg.max_grad_norm))\n",
    "    temperature_opt_transforms.insert(0, optax.clip_by_global_norm(cfg.max_grad_norm))\n",
    "opt = {\n",
    "    V: optax.chain(*v_opt_transforms),\n",
    "    POLICY: optax.chain(*policy_opt_transforms),\n",
    "    TEMPERATURE: optax.chain(*temperature_opt_transforms)\n",
    "}\n",
    "\n",
    "learner = PCL(model=model,\n",
    "              opt=opt,\n",
    "              buffer=buffer,\n",
    "              cfg=cfg)\n",
    "\n",
    "agent = RLAgent(model=model,\n",
    "                model_key=POLICY,\n",
    "                buffer=buffer,\n",
    "                learner=learner,\n",
    "                key=cfg.agent_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "952d3924",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x29e613bb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00043b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ==================================================\n",
      "Current return (episode: 74, is finished: False) with length 98: 117.12971820943532\n",
      "(DeviceArray([ 0.42114586,  0.9355575 , -0.04016141], dtype=float32), DeviceArray([0.42974144, 0.24413195, 0.3877744 ], dtype=float32))\n",
      "Evaluation:\n",
      "Mean return: 103.91280773815738, Mean length: 80.9\n",
      "Epoch 2 ==================================================\n",
      "Current return (episode: 97, is finished: False) with length 196: 307.73655747663776\n",
      "(DeviceArray([-0.11082284, -0.10101343, -0.11712176], dtype=float32), DeviceArray([0.20232834, 0.4027467 , 0.19349763], dtype=float32))\n",
      "Evaluation:\n",
      "Mean return: 378.8388659294139, Mean length: 192.9\n",
      "Epoch 3 ==================================================\n",
      "Current return (episode: 122, is finished: False) with length 78: 115.30010039124826\n",
      "(DeviceArray([-0.5212943 , -0.08940071,  0.23862752], dtype=float32), DeviceArray([0.27584276, 0.30227402, 0.42868024], dtype=float32))\n",
      "Evaluation:\n",
      "Mean return: 371.5436614242149, Mean length: 189.9\n",
      "Epoch 4 ==================================================\n",
      "Current return (episode: 147, is finished: False) with length 130: 215.9397846213762\n",
      "(DeviceArray([-0.00227779,  0.03066964, -0.22755392], dtype=float32), DeviceArray([0.3378875 , 0.46255055, 0.2890781 ], dtype=float32))\n",
      "Evaluation:\n",
      "Mean return: 349.0138617651499, Mean length: 168.6\n",
      "Epoch 5 ==================================================\n",
      "Current return (episode: 175, is finished: False) with length 92: 133.5311535908831\n",
      "(DeviceArray([ 0.4648225 , -0.11430991,  0.06093244], dtype=float32), DeviceArray([0.8017289, 0.6805365, 1.3520467], dtype=float32))\n",
      "Evaluation:\n",
      "Mean return: 413.2265119963883, Mean length: 228.2\n",
      "Epoch 6 ==================================================\n",
      "Current return (episode: 204, is finished: False) with length 31: 42.826951873706165\n",
      "(DeviceArray([ 0.1473692 ,  0.12108545, -0.18181978], dtype=float32), DeviceArray([0.4646257 , 0.47234872, 0.3262967 ], dtype=float32))\n",
      "Evaluation:\n",
      "Mean return: 343.8063366630796, Mean length: 162.7\n",
      "Epoch 7 ==================================================\n",
      "Current return (episode: 232, is finished: False) with length 134: 245.92650289444492\n",
      "(DeviceArray([ 0.03219328,  0.02115012, -0.12670471], dtype=float32), DeviceArray([0.35823554, 0.52609885, 0.31126267], dtype=float32))\n",
      "Evaluation:\n",
      "Mean return: 334.8861399660765, Mean length: 152.2\n",
      "Epoch 8 ==================================================\n",
      "Current return (episode: 262, is finished: False) with length 95: 153.01655053387947\n",
      "(DeviceArray([ 0.02594228, -0.2970281 , -0.3283607 ], dtype=float32), DeviceArray([0.52057284, 0.3981108 , 0.3238083 ], dtype=float32))\n",
      "Evaluation:\n",
      "Mean return: 344.18576247126754, Mean length: 161.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/personal/jax_learning/jax_learning/rl_utils.py:77\u001b[0m, in \u001b[0;36minteract\u001b[0;34m(env, agent, cfg)\u001b[0m\n\u001b[1;32m     75\u001b[0m render()\n\u001b[1;32m     76\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore(obs, h_state, act, rew, done, info, next_obs, next_h_state)\n\u001b[0;32m---> 77\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_h_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timestep_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat((timestep_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m log_interval) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m~/research/personal/jax_learning/jax_learning/agents/agents.py:91\u001b[0m, in \u001b[0;36mLearningAgent.learn\u001b[0;34m(self, next_obs, next_h_state, learn_info)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m, next_obs: np\u001b[38;5;241m.\u001b[39mndarray, next_h_state: np\u001b[38;5;241m.\u001b[39mndarray, learn_info: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_h_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/personal/jax_learning/jax_learning/learners/path_consistency.py:230\u001b[0m, in \u001b[0;36mPCL.learn\u001b[0;34m(self, next_obs, next_h_state, learn_info)\u001b[0m\n\u001b[1;32m    216\u001b[0m     obss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_rms\u001b[38;5;241m.\u001b[39mnormalize(obss)\n\u001b[1;32m    218\u001b[0m (obss, h_states, acts, rews, dones, lengths) \u001b[38;5;241m=\u001b[39m to_jnp(\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;241m*\u001b[39mbatch_flatten(obss, h_states, acts, rews, dones, lengths)\n\u001b[1;32m    220\u001b[0m )\n\u001b[1;32m    222\u001b[0m (\n\u001b[1;32m    223\u001b[0m     policy,\n\u001b[1;32m    224\u001b[0m     v,\n\u001b[1;32m    225\u001b[0m     policy_opt_state,\n\u001b[1;32m    226\u001b[0m     v_opt_state,\n\u001b[1;32m    227\u001b[0m     grads,\n\u001b[1;32m    228\u001b[0m     pc_learn_info,\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_key,\n\u001b[0;32m--> 230\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPOLICY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mV\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPOLICY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mV\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_opt_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPOLICY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_opt_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mV\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msample_idxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msample_idxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43macts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msample_idxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrews\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrews\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msample_idxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model[POLICY], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model[V] \u001b[38;5;241m=\u001b[39m policy, v\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opt_state[POLICY], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opt_state[V] \u001b[38;5;241m=\u001b[39m policy_opt_state, v_opt_state\n",
      "File \u001b[0;32m~/research/personal/jax_learning/.venv/lib/python3.10/site-packages/equinox/jit.py:94\u001b[0m, in \u001b[0;36m_JitWrapper.__call__\u001b[0;34m(_JitWrapper__self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(__self, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__self\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/personal/jax_learning/.venv/lib/python3.10/site-packages/equinox/jit.py:90\u001b[0m, in \u001b[0;36m_JitWrapper._fun_wrapper\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached\u001b[38;5;241m.\u001b[39mlower(dynamic, static)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     dynamic_out, static_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combine(dynamic_out, static_out\u001b[38;5;241m.\u001b[39mvalue)\n",
      "File \u001b[0;32m~/research/personal/jax_learning/.venv/lib/python3.10/site-packages/equinox/module.py:263\u001b[0m, in \u001b[0;36mModule.tree_unflatten\u001b[0;34m(cls, aux, dynamic_field_values)\u001b[0m\n\u001b[1;32m    256\u001b[0m             dynamic_field_values\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(dynamic_field_values), (\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(dynamic_field_names),\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(static_field_names),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(static_field_values),\n\u001b[1;32m    261\u001b[0m     )\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_unflatten\u001b[39m(\u001b[38;5;28mcls\u001b[39m, aux, dynamic_field_values):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    266\u001b[0m     dynamic_field_names, static_field_names, static_field_values \u001b[38;5;241m=\u001b[39m aux\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "interact(env, agent, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f03ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
