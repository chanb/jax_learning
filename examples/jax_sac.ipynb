{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3ba0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import sys\n",
    "import timeit\n",
    "import wandb\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "from jax import grad, jit, vmap\n",
    "from typing import Sequence, Tuple, Optional, Callable, Dict\n",
    "\n",
    "from jax_learning.agents.rl_agents import RLAgent\n",
    "from jax_learning.buffers import ReplayBuffer\n",
    "from jax_learning.buffers.ram_buffers import NextStateNumPyBuffer\n",
    "from jax_learning.buffers.utils import batch_flatten, to_jnp\n",
    "from jax_learning.common import polyak_average_generator\n",
    "from jax_learning.constants import DISCRETE, CONTINUOUS\n",
    "from jax_learning.rl_utils import interact, evaluate\n",
    "from jax_learning.learners import LearnerWithTargetNetwork\n",
    "from jax_learning.models import StochasticPolicy, ActionValue, Temperature\n",
    "from jax_learning.models.policies import MLPSquashedGaussianPolicy\n",
    "from jax_learning.models.q_functions import MLPQ, MultiQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a46b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:47: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/chanb/work/personal_research/jax_learning/examples/wandb/run-20220627_204245-8zw3k2xh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/chan/test_jax_rl/runs/8zw3k2xh\" target=\"_blank\">flowing-shadow-52</a></strong> to <a href=\"http://localhost:8080/chan/test_jax_rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x1126af340>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"test_jax_rl\", group=\"reacher-sac_test\")\n",
    "wandb.define_metric(\"episodic_return\", summary=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68d3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\n",
    "    # Environment setup\n",
    "    \"env\": \"Reacher-v2\",\n",
    "    \"seed\": 0,\n",
    "    \"render\": False,\n",
    "    \n",
    "    # Experiment progress\n",
    "    \"load_step\": 0,\n",
    "    \"log_interval\": 5000,\n",
    "    \n",
    "    # Learning hyperparameters\n",
    "    \"max_timesteps\": 1000000,\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"buffer_warmup\": 1000,\n",
    "    \"num_gradient_steps\": 1,\n",
    "    \"batch_size\": 128,\n",
    "    \"max_grad_norm\": 10.,\n",
    "    \"gamma\": 0.99,\n",
    "    \"update_frequency\": 1,\n",
    "    \n",
    "    # Actor\n",
    "    \"actor_lr\": 3e-4,\n",
    "    \"actor_update_frequency\": 1,\n",
    "    \n",
    "    # Critic\n",
    "    \"critic_lr\": 3e-4,\n",
    "    \"target_update_frequency\": 1,\n",
    "    \"tau\": 0.01, # This is for polyak averaging of target network\n",
    "    \n",
    "    # Normalization\n",
    "    \"normalize_obs\": False,\n",
    "    \"normalize_value\": False,\n",
    "    \n",
    "    # Temperature\n",
    "    \"alpha_lr\": 3e-4,\n",
    "    \"init_alpha\": 1.0,\n",
    "    \"target_entropy\": \"auto\",\n",
    "    \n",
    "    # Model architecture\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_hidden\": 2,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"eval_cfg\": {\n",
    "        \"max_episodes\": 100,\n",
    "        \"seed\": 1,\n",
    "        \"render\": True,\n",
    "    }\n",
    "}\n",
    "cfg = Namespace(**cfg_dict)\n",
    "eval_cfg = Namespace(**cfg.eval_cfg)\n",
    "wandb.config = cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733f2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "502213a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/envs/registration.py:564: UserWarning: \u001b[33mWARN: The environment Reacher-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:46: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "objc[31153]: Class GLFWWindowDelegate is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x1222177b0) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x1222aa700). One of the two will be used. Which one is undefined.\n",
      "objc[31153]: Class GLFWApplicationDelegate is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x122217788) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x1222aa778). One of the two will be used. Which one is undefined.\n",
      "objc[31153]: Class GLFWContentView is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x122217800) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x1222aa7a0). One of the two will be used. Which one is undefined.\n",
      "objc[31153]: Class GLFWWindow is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x122217878) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x1222aa818). One of the two will be used. Which one is undefined.\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:69: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:73: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(cfg.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ed36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.obs_dim = env.observation_space.shape\n",
    "cfg.act_dim = env.action_space.shape\n",
    "if cfg.target_entropy == \"auto\":\n",
    "    cfg.target_entropy = -float(np.product(env.action_space.shape))\n",
    "cfg.action_space = CONTINUOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.h_state_dim = (1,)\n",
    "cfg.rew_dim = (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a581441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(env='Reacher-v2', seed=0, render=False, load_step=0, log_interval=5000, max_timesteps=1000000, buffer_size=1000000, buffer_warmup=1000, num_gradient_steps=1, batch_size=128, max_grad_norm=10.0, gamma=0.99, update_frequency=1, actor_lr=0.0003, actor_update_frequency=1, critic_lr=0.0003, target_update_frequency=1, tau=0.01, normalize_obs=False, normalize_value=False, alpha_lr=0.0003, init_alpha=1.0, target_entropy=-2.0, hidden_dim=256, num_hidden=2, eval_cfg={'max_episodes': 100, 'seed': 1, 'render': True}, obs_dim=(11,), act_dim=(2,), action_space='continuous', h_state_dim=(1,), rew_dim=(1,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c05894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "cfg.buffer_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.env_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.agent_key, cfg.model_key = jrandom.split(jrandom.PRNGKey(cfg.seed), num=2)\n",
    "eval_cfg.env_rng = np.random.RandomState(eval_cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83fd4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_LOSS = \"q_loss\"\n",
    "POLICY_LOSS = \"policy_loss\"\n",
    "TEMPERATURE_LOSS = \"temperature_loss\"\n",
    "MEAN_Q_LOSS = \"mean_q_loss\"\n",
    "MEAN_POLICY_LOSS = \"mean_policy_loss\"\n",
    "MEAN_TEMPERATURE_LOSS = \"mean_temperature_loss\"\n",
    "MEAN_CURR_Q = \"mean_curr_q\"\n",
    "MEAN_NEXT_Q = \"mean_next_q\"\n",
    "MAX_CURR_Q = \"max_curr_q\"\n",
    "MAX_NEXT_Q = \"max_next_q\"\n",
    "MIN_CURR_Q = \"min_curr_q\"\n",
    "MIN_NEXT_Q = \"min_next_q\"\n",
    "MAX_TD_ERROR = \"max_td_error\"\n",
    "MIN_TD_ERROR = \"min_td_error\"\n",
    "POLICY = \"policy\"\n",
    "Q = \"q\"\n",
    "TEMPERATURE = \"temperature\"\n",
    "TARGET_ENTROPY = \"target_entropy\"\n",
    "\n",
    "def clipped_min_q_td_error(curr_q_pred: np.ndarray,\n",
    "                           next_q_pred_min: np.ndarray,\n",
    "                           next_lprob: np.ndarray,\n",
    "                           rew: np.ndarray,\n",
    "                           done: np.ndarray,\n",
    "                           temp: float,\n",
    "                           gamma: float) -> np.ndarray:\n",
    "    v_next = (next_q_pred_min - temp * next_lprob)\n",
    "    curr_q_target = rew + gamma * (1 - done) * v_next\n",
    "    return curr_q_pred - curr_q_target\n",
    "\n",
    "def sac_policy_loss(curr_q_pred_min: np.ndarray,\n",
    "                    lprob: np.ndarray,\n",
    "                    temp: float) -> np.ndarray:\n",
    "    return -(curr_q_pred_min - temp * lprob)\n",
    "\n",
    "def sac_temperature_loss(temp: float,\n",
    "                         lprob: np.ndarray,\n",
    "                         target_entropy: float) -> np.ndarray:\n",
    "    return temp * -(lprob + target_entropy)\n",
    "\n",
    "\n",
    "class SAC(LearnerWithTargetNetwork):\n",
    "    def __init__(self,\n",
    "                 model: Dict[str, eqx.Module],\n",
    "                 target_model: Dict[str, eqx.Module],\n",
    "                 opt: Dict[str, optax.GradientTransformation],\n",
    "                 buffer: ReplayBuffer,\n",
    "                 cfg: Namespace):\n",
    "        super().__init__(model, target_model, opt, buffer, cfg)\n",
    "        \n",
    "        self._batch_size = cfg.batch_size\n",
    "        self._num_gradient_steps = cfg.num_gradient_steps\n",
    "        \n",
    "        self._buffer_warmup = cfg.buffer_warmup\n",
    "        self._actor_update_frequency = cfg.actor_update_frequency\n",
    "        self._target_update_frequency = cfg.target_update_frequency\n",
    "        \n",
    "        self._target_entropy = getattr(cfg, TARGET_ENTROPY, None)\n",
    "        self._sample_key = jrandom.PRNGKey(cfg.seed)\n",
    "        \n",
    "        _clipped_min_q_td_error = jax.vmap(clipped_min_q_td_error, in_axes=[0, 0, 0, 0, 0, None, None])\n",
    "\n",
    "        @eqx.filter_grad(has_aux=True)\n",
    "        def q_loss(models: Tuple[ActionValue, ActionValue],\n",
    "                   policy: StochasticPolicy,\n",
    "                   temperature: Temperature,\n",
    "                   obss: np.ndarray,\n",
    "                   h_states: np.ndarray,\n",
    "                   acts: np.ndarray,\n",
    "                   rews: np.ndarray,\n",
    "                   dones: np.ndarray,\n",
    "                   next_obss: np.ndarray,\n",
    "                   next_h_states: np.ndarray,\n",
    "                   keys: Sequence[jrandom.PRNGKey]) -> Tuple[np.ndarray, dict]:\n",
    "            (q, target_q) = models\n",
    "            \n",
    "            curr_xs = jnp.concatenate((obss, acts), axis=-1)\n",
    "            curr_q_preds, _ = jax.vmap(q.q_values)(curr_xs, h_states)\n",
    "            \n",
    "            next_acts, next_lprobs, _ = jax.vmap(policy.act_lprob)(next_obss, next_h_states, keys)\n",
    "            next_lprobs = jnp.sum(next_lprobs, axis=-1, keepdims=True)\n",
    "            \n",
    "            next_xs = jnp.concatenate((next_obss, next_acts), axis=-1)\n",
    "            next_q_preds, _ = jax.vmap(target_q.q_values)(next_xs, next_h_states)\n",
    "            next_q_preds_min = jnp.min(next_q_preds, axis=1)\n",
    "            \n",
    "            temp = temperature()\n",
    "            \n",
    "            def batch_td_errors(curr_q_pred):\n",
    "                return _clipped_min_q_td_error(curr_q_pred,\n",
    "                                               next_q_preds_min,\n",
    "                                               next_lprobs,\n",
    "                                               rews,\n",
    "                                               dones,\n",
    "                                               temp,\n",
    "                                               self._gamma)\n",
    "            td_errors = jax.vmap(batch_td_errors, in_axes=[1])(curr_q_preds)\n",
    "            loss = jnp.mean(td_errors ** 2)\n",
    "            return loss, {\n",
    "                Q_LOSS: loss,\n",
    "                MAX_NEXT_Q: jnp.max(next_q_preds),\n",
    "                MIN_NEXT_Q: jnp.min(next_q_preds),\n",
    "                MEAN_NEXT_Q: jnp.mean(next_q_preds),\n",
    "                MAX_CURR_Q: jnp.max(curr_q_preds),\n",
    "                MIN_CURR_Q: jnp.min(curr_q_preds),\n",
    "                MEAN_CURR_Q: jnp.mean(curr_q_preds),\n",
    "                MAX_TD_ERROR: jnp.max(td_errors),\n",
    "                MIN_TD_ERROR: jnp.min(td_errors),\n",
    "            }\n",
    "        \n",
    "        apply_residual_gradient = polyak_average_generator(getattr(cfg, \"omega\", 1.0))\n",
    "        \n",
    "        def update_q(q: ActionValue,\n",
    "                     target_q: ActionValue,\n",
    "                     policy: StochasticPolicy,\n",
    "                     temperature: Temperature,\n",
    "                     opt: optax.GradientTransformation,\n",
    "                     opt_state: optax.OptState,\n",
    "                     obss: np.ndarray,\n",
    "                     h_states: np.ndarray,\n",
    "                     acts: np.ndarray,\n",
    "                     rews: np.ndarray,\n",
    "                     dones: np.ndarray,\n",
    "                     next_obss: np.ndarray,\n",
    "                     next_h_states: np.ndarray) -> Tuple[ActionValue,\n",
    "                                                         optax.OptState,\n",
    "                                                         Tuple[jax.tree_util.PyTreeDef,\n",
    "                                                               jax.tree_util.PyTreeDef,\n",
    "                                                               jax.tree_util.PyTreeDef],\n",
    "                                                         dict,\n",
    "                                                         jrandom.PRNGKey]:\n",
    "            sample_key = jrandom.split(self._sample_key, num=1)[0]\n",
    "            keys = jrandom.split(self._sample_key, num=self._batch_size)\n",
    "            grads, learn_info = q_loss((q, target_q),\n",
    "                                       policy,\n",
    "                                       temperature,\n",
    "                                       obss,\n",
    "                                       h_states,\n",
    "                                       acts,\n",
    "                                       rews,\n",
    "                                       dones,\n",
    "                                       next_obss,\n",
    "                                       next_h_states,\n",
    "                                       keys)\n",
    "\n",
    "            (q_grads, target_q_grads) = grads\n",
    "            grads = jax.tree_map(apply_residual_gradient,\n",
    "                                 q_grads,\n",
    "                                 target_q_grads)\n",
    "\n",
    "            updates, opt_state = opt.update(grads, opt_state)\n",
    "            q = eqx.apply_updates(q, updates)\n",
    "            return q, opt_state, (grads, q_grads, target_q_grads), learn_info, sample_key\n",
    "\n",
    "        _sac_policy_loss = jax.vmap(sac_policy_loss, in_axes=[0, 0, None])\n",
    "        \n",
    "        @eqx.filter_grad(has_aux=True)\n",
    "        def policy_loss(policy: StochasticPolicy,\n",
    "                        q: ActionValue,\n",
    "                        temperature: Temperature,\n",
    "                        obss: np.ndarray,\n",
    "                        h_states: np.ndarray,\n",
    "                        keys: Sequence[jrandom.PRNGKey]) -> Tuple[np.ndarray, dict]:\n",
    "            acts, lprobs, _ = jax.vmap(policy.act_lprob)(obss, h_states, keys)\n",
    "            lprobs = jnp.sum(lprobs, axis=-1, keepdims=True)\n",
    "            curr_xs = jnp.concatenate((obss, acts), axis=-1)\n",
    "            curr_q_preds, _ = jax.vmap(q.q_values)(curr_xs, h_states)\n",
    "            curr_q_preds_min = jnp.min(curr_q_preds, axis=1)\n",
    "            temp = temperature()\n",
    "            \n",
    "            loss = jnp.mean(_sac_policy_loss(curr_q_preds_min, lprobs, temp))\n",
    "            return loss, {\n",
    "                POLICY_LOSS: loss,\n",
    "            }\n",
    "        \n",
    "        def update_policy(policy: StochasticPolicy,\n",
    "                          q: ActionValue,\n",
    "                          temperature: Temperature,\n",
    "                          opt: optax.GradientTransformation,\n",
    "                          opt_state: optax.OptState,\n",
    "                          obss: np.ndarray,\n",
    "                          h_states: np.ndarray,\n",
    "                          acts: np.ndarray) -> Tuple[ActionValue,\n",
    "                                                     optax.OptState,\n",
    "                                                     jax.tree_util.PyTreeDef,\n",
    "                                                     dict,\n",
    "                                                     jrandom.PRNGKey]:\n",
    "            sample_key = jrandom.split(self._sample_key, num=1)[0]\n",
    "            keys = jrandom.split(self._sample_key, num=self._batch_size)\n",
    "            \n",
    "            grads, learn_info = policy_loss(policy,\n",
    "                                            q,\n",
    "                                            temperature,\n",
    "                                            obss,\n",
    "                                            h_states,\n",
    "                                            keys)\n",
    "\n",
    "            updates, opt_state = opt.update(grads, opt_state)\n",
    "            policy = eqx.apply_updates(policy, updates)\n",
    "            return policy, opt_state, grads, learn_info, sample_key\n",
    "\n",
    "        _sac_temperature_loss = jax.vmap(sac_temperature_loss, in_axes=[None, 0, None])\n",
    "        \n",
    "        @eqx.filter_grad(has_aux=True)\n",
    "        def temperature_loss(temperature: Temperature,\n",
    "                             policy: StochasticPolicy,\n",
    "                             obss: np.ndarray,\n",
    "                             h_states: np.ndarray,\n",
    "                             keys: Sequence[jrandom.PRNGKey]) -> Tuple[np.ndarray, dict]:\n",
    "            temp = temperature()\n",
    "            _, lprobs, _ = jax.vmap(policy.act_lprob)(obss, h_states, keys)\n",
    "            lprobs = jnp.sum(lprobs, axis=-1, keepdims=True)\n",
    "            loss = jnp.mean(_sac_temperature_loss(temp, lprobs, self._target_entropy))\n",
    "            return loss, {\n",
    "                TEMPERATURE_LOSS: loss,\n",
    "            }\n",
    "        \n",
    "        def update_temperature(policy: StochasticPolicy,\n",
    "                               temperature: Temperature,\n",
    "                               opt: optax.GradientTransformation,\n",
    "                               opt_state: optax.OptState,\n",
    "                               obss: np.ndarray,\n",
    "                               h_states: np.ndarray) -> Tuple[ActionValue,\n",
    "                                                              optax.OptState,\n",
    "                                                              jax.tree_util.PyTreeDef,\n",
    "                                                              dict,\n",
    "                                                              jrandom.PRNGKey]:\n",
    "            sample_key = jrandom.split(self._sample_key, num=1)[0]\n",
    "            keys = jrandom.split(self._sample_key, num=self._batch_size)\n",
    "            grads, learn_info = temperature_loss(temperature,\n",
    "                                                 policy,\n",
    "                                                 obss,\n",
    "                                                 h_states,\n",
    "                                                 keys)\n",
    "\n",
    "            updates, opt_state = opt.update(grads, opt_state)\n",
    "            temperature = eqx.apply_updates(temperature, updates)\n",
    "            return temperature, opt_state, grads, learn_info, sample_key\n",
    "        \n",
    "        self.update_q = eqx.filter_jit(update_q)\n",
    "        self.update_policy = eqx.filter_jit(update_policy)\n",
    "        self.update_temperature = eqx.filter_jit(update_temperature)\n",
    "        \n",
    "    def learn(self, next_obs: np.ndarray, next_h_state: np.ndarray, learn_info: dict):\n",
    "        self._step += 1\n",
    "\n",
    "        if (\n",
    "            self._step <= self._buffer_warmup\n",
    "            or (self._step - 1 - self._buffer_warmup) % self._update_frequency != 0\n",
    "        ):\n",
    "            return\n",
    "\n",
    "        learn_info[MEAN_Q_LOSS] = 0.0\n",
    "        learn_info[MEAN_POLICY_LOSS] = 0.0\n",
    "        learn_info[MEAN_TEMPERATURE_LOSS] = 0.0\n",
    "        learn_info[MEAN_CURR_Q] = 0.0\n",
    "        learn_info[MEAN_NEXT_Q] = 0.0\n",
    "        learn_info[MAX_CURR_Q] = -np.inf\n",
    "        learn_info[MAX_NEXT_Q] = -np.inf\n",
    "        learn_info[MIN_CURR_Q] = np.inf\n",
    "        learn_info[MIN_NEXT_Q] = np.inf\n",
    "        for update_i in range(self._num_gradient_steps):\n",
    "            (\n",
    "                obss,\n",
    "                h_states,\n",
    "                acts,\n",
    "                rews,\n",
    "                dones,\n",
    "                next_obss,\n",
    "                next_h_states,\n",
    "                _,\n",
    "                _,\n",
    "                _,\n",
    "            ) = self.buffer.sample_with_next_obs(\n",
    "                batch_size=self._batch_size,\n",
    "                next_obs=next_obs,\n",
    "                next_h_state=next_h_state,\n",
    "            )\n",
    "\n",
    "            if self.obs_rms:\n",
    "                obss = self.obs_rms.normalize(obss)\n",
    "\n",
    "            (obss, h_states, acts, rews, dones, next_obss, next_h_states) = to_jnp(\n",
    "                *batch_flatten(\n",
    "                    obss, h_states, acts, rews, dones, next_obss, next_h_states\n",
    "                )\n",
    "            )\n",
    "            q, opt_state, grads, q_learn_info, self._sample_key = self.update_q(\n",
    "                q=self.model[Q],\n",
    "                target_q=self.target_model[Q],\n",
    "                policy=self.model[POLICY],\n",
    "                temperature=self.model[TEMPERATURE],\n",
    "                opt=self.opt[Q],\n",
    "                opt_state=self.opt_state[Q],\n",
    "                obss=obss,\n",
    "                h_states=h_states,\n",
    "                acts=acts,\n",
    "                rews=rews,\n",
    "                dones=dones,\n",
    "                next_obss=next_obss,\n",
    "                next_h_states=next_h_states,\n",
    "            )\n",
    "\n",
    "            self._model[Q] = q\n",
    "            self._opt_state[Q] = opt_state\n",
    "            \n",
    "            if self._step % self._actor_update_frequency == 0:\n",
    "                policy, opt_state, grads, policy_learn_info, self._sample_key = self.update_policy(\n",
    "                    policy=self.model[POLICY],\n",
    "                    q=self.model[Q],\n",
    "                    temperature=self.model[TEMPERATURE],\n",
    "                    opt=self.opt[POLICY],\n",
    "                    opt_state=self.opt_state[POLICY],\n",
    "                    obss=obss,\n",
    "                    h_states=h_states,\n",
    "                    acts=acts\n",
    "                )\n",
    "                self._model[POLICY] = policy\n",
    "                self._opt_state[POLICY] = opt_state\n",
    "\n",
    "                if self._target_entropy is not None:\n",
    "                    temperature, opt_state, grads, temperature_learn_info, self._sample_key = self.update_temperature(\n",
    "                        policy=self.model[POLICY],\n",
    "                        temperature=self.model[TEMPERATURE],\n",
    "                        opt=self.opt[TEMPERATURE],\n",
    "                        opt_state=self.opt_state[TEMPERATURE],\n",
    "                        obss=obss,\n",
    "                        h_states=h_states,\n",
    "                    )\n",
    "                    self._model[TEMPERATURE] = temperature\n",
    "                    self._opt_state[TEMPERATURE] = opt_state\n",
    "\n",
    "            if self._step % self._target_update_frequency == 0:\n",
    "                self.update_target_model(model_key=Q)\n",
    "\n",
    "            learn_info[MEAN_Q_LOSS] += (\n",
    "                q_learn_info[Q_LOSS].item() / self._num_gradient_steps\n",
    "            )\n",
    "            learn_info[MEAN_POLICY_LOSS] += (\n",
    "                policy_learn_info[POLICY_LOSS].item() / self._num_gradient_steps\n",
    "            )\n",
    "            learn_info[MEAN_TEMPERATURE_LOSS] += (\n",
    "                temperature_learn_info[TEMPERATURE_LOSS].item() / self._num_gradient_steps\n",
    "            )\n",
    "            learn_info[MEAN_CURR_Q] += (\n",
    "                q_learn_info[MEAN_CURR_Q].item() / self._num_gradient_steps\n",
    "            )\n",
    "            learn_info[MEAN_NEXT_Q] += (\n",
    "                q_learn_info[MEAN_NEXT_Q].item() / self._num_gradient_steps\n",
    "            )\n",
    "            learn_info[MAX_CURR_Q] = max(\n",
    "                q_learn_info[MAX_CURR_Q], q_learn_info[MAX_CURR_Q].item()\n",
    "            )\n",
    "            learn_info[MAX_NEXT_Q] = max(\n",
    "                q_learn_info[MAX_NEXT_Q], q_learn_info[MAX_NEXT_Q].item()\n",
    "            )\n",
    "            learn_info[MIN_CURR_Q] = min(\n",
    "                q_learn_info[MIN_CURR_Q], q_learn_info[MIN_CURR_Q].item()\n",
    "            )\n",
    "            learn_info[MIN_NEXT_Q] = min(\n",
    "                q_learn_info[MIN_NEXT_Q], q_learn_info[MIN_NEXT_Q].item()\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e72cdf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "buffer = NextStateNumPyBuffer(\n",
    "    buffer_size=cfg.buffer_size,\n",
    "    obs_dim=cfg.obs_dim,\n",
    "    h_state_dim=cfg.h_state_dim,\n",
    "    act_dim=(1,) if cfg.action_space == DISCRETE else cfg.act_dim,\n",
    "    rew_dim=cfg.rew_dim,\n",
    "    rng=cfg.buffer_rng,\n",
    ")\n",
    "\n",
    "policy_key, q_key = jrandom.split(cfg.model_key)\n",
    "policy = MLPSquashedGaussianPolicy(\n",
    "        obs_dim=cfg.obs_dim,\n",
    "        act_dim=cfg.act_dim,\n",
    "        hidden_dim=cfg.hidden_dim,\n",
    "        num_hidden=cfg.num_hidden,\n",
    "        key=policy_key,\n",
    ")\n",
    "\n",
    "temperature = Temperature(init_alpha=cfg.init_alpha)\n",
    "\n",
    "q_constructor = partial(MLPQ,\n",
    "                        in_dim=(cfg.obs_dim[0] + cfg.act_dim[0],),\n",
    "                        out_dim=(1,),\n",
    "                        hidden_dim=cfg.hidden_dim,\n",
    "                        num_hidden=cfg.num_hidden)\n",
    "\n",
    "q = MultiQ(q_constructor,\n",
    "           num_qs=2,\n",
    "           key=q_key)\n",
    "\n",
    "target_q = MultiQ(q_constructor,\n",
    "                  num_qs=2,\n",
    "                  key=q_key)\n",
    "\n",
    "model = {\n",
    "    POLICY: policy,\n",
    "    TEMPERATURE: temperature,\n",
    "    Q: q,\n",
    "}\n",
    "\n",
    "target_model = {\n",
    "    Q: target_q\n",
    "}\n",
    "\n",
    "q_opt_transforms = [\n",
    "    optax.scale_by_adam(),\n",
    "    optax.scale(-cfg.critic_lr)\n",
    "]\n",
    "\n",
    "policy_opt_transforms = [\n",
    "    optax.scale_by_adam(),\n",
    "    optax.scale(-cfg.actor_lr)\n",
    "]\n",
    "\n",
    "temperature_opt_transforms = [\n",
    "    optax.scale_by_adam(),\n",
    "    optax.scale(-cfg.alpha_lr)\n",
    "]\n",
    "\n",
    "if cfg.max_grad_norm:\n",
    "    q_opt_transforms.insert(0, optax.clip_by_global_norm(cfg.max_grad_norm))\n",
    "    policy_opt_transforms.insert(0, optax.clip_by_global_norm(cfg.max_grad_norm))\n",
    "    temperature_opt_transforms.insert(0, optax.clip_by_global_norm(cfg.max_grad_norm))\n",
    "opt = {\n",
    "    Q: optax.chain(*q_opt_transforms),\n",
    "    POLICY: optax.chain(*policy_opt_transforms),\n",
    "    TEMPERATURE: optax.chain(*temperature_opt_transforms)\n",
    "}\n",
    "\n",
    "learner = SAC(model=model,\n",
    "              target_model=target_model,\n",
    "              opt=opt,\n",
    "              buffer=buffer,\n",
    "              cfg=cfg)\n",
    "\n",
    "agent = RLAgent(model=model,\n",
    "                model_key=POLICY,\n",
    "                buffer=buffer,\n",
    "                learner=learner,\n",
    "                key=cfg.agent_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "952d3924",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://localhost:8080/chan/test_jax_rl/runs/8zw3k2xh?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x142a3cf70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00043b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interact(env, agent, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32851f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env, agent, eval_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f03ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95e38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
