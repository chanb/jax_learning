{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3ba0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import timeit\n",
    "import wandb\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "\n",
    "from jax_learning.agents.rl_agents import RLAgent\n",
    "from jax_learning.buffers.ram_buffers import NextStateNumPyBuffer\n",
    "from jax_learning.common import init_wandb\n",
    "from jax_learning.constants import DISCRETE, CONTINUOUS\n",
    "from jax_learning.rl_utils import interact, evaluate, random_exploration_generator\n",
    "from jax_learning.learners.soft_actor_critic import SAC\n",
    "from jax_learning.models import Temperature\n",
    "from jax_learning.models.policies import MLPSquashedGaussianPolicy\n",
    "from jax_learning.models.q_functions import MLPQ, MultiQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a46b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:47: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/chanb/work/personal_research/jax_learning/examples/wandb/run-20220629_174818-3ld2gt8v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/chan/test_jax_rl/runs/3ld2gt8v\" target=\"_blank\">skilled-durian-122</a></strong> to <a href=\"http://localhost:8080/chan/test_jax_rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_wandb(project=\"test_jax_rl\", group=\"hopper-sac_test\", mode=\"online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68d3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\n",
    "    # Environment setup\n",
    "    \"env\": \"Hopper-v2\",\n",
    "    \"seed\": 0,\n",
    "    \"render\": False,\n",
    "    \n",
    "    # Experiment progress\n",
    "    \"load_step\": 0,\n",
    "    \"log_interval\": 5000,\n",
    "    \n",
    "    # Learning hyperparameters\n",
    "    \"max_timesteps\": 1000000,\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"buffer_warmup\": 1000,\n",
    "    \"num_gradient_steps\": 1,\n",
    "    \"batch_size\": 256,\n",
    "    \"max_grad_norm\": False,\n",
    "    \"gamma\": 0.99,\n",
    "    \"update_frequency\": 1,\n",
    "    \n",
    "    \"exploration_steps\": 1000,\n",
    "    \"exploration_strategy\": \"standard_gaussian\",\n",
    "    \n",
    "    # Actor\n",
    "    \"actor_lr\": 3e-4,\n",
    "    \"actor_update_frequency\": 1,\n",
    "    \n",
    "    # Critic\n",
    "    \"critic_lr\": 3e-4,\n",
    "    \"target_update_frequency\": 1,\n",
    "    \"tau\": 0.005, # This is for polyak averaging of target network\n",
    "    \n",
    "    # Normalization\n",
    "    \"normalize_obs\": False,\n",
    "    \"normalize_value\": False,\n",
    "    \n",
    "    # Temperature\n",
    "    \"alpha_lr\": 3e-4,\n",
    "    \"init_alpha\": 0.2,\n",
    "    \"target_entropy\": None,\n",
    "    \n",
    "    # Model architecture\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_hidden\": 2,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"evaluation_frequency\": 5000,\n",
    "    \"eval_cfg\": {\n",
    "        \"num_episodes\": 10,\n",
    "        \"seed\": 1,\n",
    "        \"render\": True,\n",
    "        \"clip_action\": True,\n",
    "        \"max_action\": 1.,\n",
    "        \"min_action\": -1.,\n",
    "    }\n",
    "}\n",
    "cfg = Namespace(**cfg_dict)\n",
    "eval_cfg = Namespace(**cfg.eval_cfg)\n",
    "wandb.config = cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733f2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "502213a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/envs/registration.py:564: UserWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:46: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "objc[15346]: Class GLFWWindowDelegate is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x1141ec7b0) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x118f7d700). One of the two will be used. Which one is undefined.\n",
      "objc[15346]: Class GLFWApplicationDelegate is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x1141ec788) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x118f7d778). One of the two will be used. Which one is undefined.\n",
      "objc[15346]: Class GLFWContentView is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x1141ec800) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x118f7d7a0). One of the two will be used. Which one is undefined.\n",
      "objc[15346]: Class GLFWWindow is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x1141ec878) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x118f7d818). One of the two will be used. Which one is undefined.\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:69: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:73: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(cfg.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ed36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.obs_dim = env.observation_space.shape\n",
    "cfg.act_dim = env.action_space.shape\n",
    "if cfg.target_entropy == \"auto\":\n",
    "    cfg.target_entropy = -float(np.product(env.action_space.shape))\n",
    "cfg.action_space = CONTINUOUS\n",
    "\n",
    "cfg.random_exploration = None\n",
    "if getattr(cfg ,\"exploration_steps\", False):\n",
    "    cfg.random_exploration = random_exploration_generator(cfg.exploration_strategy,\n",
    "                                                          cfg.act_dim,\n",
    "                                                          getattr(cfg, \"min_action\", -1.),\n",
    "                                                          getattr(cfg, \"max_action\", 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.h_state_dim = (1,)\n",
    "cfg.rew_dim = (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.buffer_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.env_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.agent_key, cfg.model_key = jrandom.split(jrandom.PRNGKey(cfg.seed), num=2)\n",
    "eval_cfg.env_rng = np.random.RandomState(eval_cfg.seed)\n",
    "cfg.evaluation_cfg = eval_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a581441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(env='Hopper-v2', seed=0, render=False, load_step=0, log_interval=5000, max_timesteps=1000000, buffer_size=1000000, buffer_warmup=1000, num_gradient_steps=1, batch_size=256, max_grad_norm=False, gamma=0.99, update_frequency=1, exploration_steps=1000, exploration_strategy='standard_gaussian', actor_lr=0.0003, actor_update_frequency=1, critic_lr=0.0003, target_update_frequency=1, tau=0.005, normalize_obs=False, normalize_value=False, alpha_lr=0.0003, init_alpha=0.2, target_entropy=None, hidden_dim=256, num_hidden=2, eval_cfg={'max_episodes': 100, 'seed': 1, 'render': True}, obs_dim=(11,), act_dim=(3,), action_space='continuous', random_exploration=<function random_exploration_generator.<locals>.sample_standard_gaussian at 0x11a919750>, h_state_dim=(1,), rew_dim=(1,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e72cdf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "POLICY = \"policy\"\n",
    "Q = \"q\"\n",
    "TEMPERATURE = \"temperature\"\n",
    "\n",
    "buffer = NextStateNumPyBuffer(\n",
    "    buffer_size=cfg.buffer_size,\n",
    "    obs_dim=cfg.obs_dim,\n",
    "    h_state_dim=cfg.h_state_dim,\n",
    "    act_dim=(1,) if cfg.action_space == DISCRETE else cfg.act_dim,\n",
    "    rew_dim=cfg.rew_dim,\n",
    "    rng=cfg.buffer_rng,\n",
    ")\n",
    "\n",
    "policy_key, q_key = jrandom.split(cfg.model_key)\n",
    "policy = MLPSquashedGaussianPolicy(\n",
    "        obs_dim=cfg.obs_dim,\n",
    "        act_dim=cfg.act_dim,\n",
    "        hidden_dim=cfg.hidden_dim,\n",
    "        num_hidden=cfg.num_hidden,\n",
    "        key=policy_key,\n",
    ")\n",
    "\n",
    "temperature = Temperature(init_alpha=cfg.init_alpha)\n",
    "\n",
    "q_constructor = partial(MLPQ,\n",
    "                        in_dim=(cfg.obs_dim[0] + cfg.act_dim[0],),\n",
    "                        out_dim=(1,),\n",
    "                        hidden_dim=cfg.hidden_dim,\n",
    "                        num_hidden=cfg.num_hidden)\n",
    "\n",
    "q = MultiQ(q_constructor,\n",
    "           num_qs=2,\n",
    "           key=q_key)\n",
    "\n",
    "target_q = MultiQ(q_constructor,\n",
    "                  num_qs=2,\n",
    "                  key=q_key)\n",
    "\n",
    "model = {\n",
    "    POLICY: policy,\n",
    "    TEMPERATURE: temperature,\n",
    "    Q: q,\n",
    "}\n",
    "\n",
    "target_model = {\n",
    "    Q: target_q\n",
    "}\n",
    "\n",
    "q_opt_transforms = [\n",
    "    optax.scale_by_adam(),\n",
    "    optax.scale(-cfg.critic_lr)\n",
    "]\n",
    "\n",
    "policy_opt_transforms = [\n",
    "    optax.scale_by_adam(),\n",
    "    optax.scale(-cfg.actor_lr)\n",
    "]\n",
    "\n",
    "temperature_opt_transforms = [\n",
    "    optax.scale_by_adam(),\n",
    "    optax.scale(-cfg.alpha_lr)\n",
    "]\n",
    "\n",
    "if cfg.max_grad_norm:\n",
    "    q_opt_transforms.insert(0, optax.clip_by_global_norm(cfg.max_grad_norm))\n",
    "    policy_opt_transforms.insert(0, optax.clip_by_global_norm(cfg.max_grad_norm))\n",
    "    temperature_opt_transforms.insert(0, optax.clip_by_global_norm(cfg.max_grad_norm))\n",
    "opt = {\n",
    "    Q: optax.chain(*q_opt_transforms),\n",
    "    POLICY: optax.chain(*policy_opt_transforms),\n",
    "    TEMPERATURE: optax.chain(*temperature_opt_transforms)\n",
    "}\n",
    "\n",
    "learner = SAC(model=model,\n",
    "              target_model=target_model,\n",
    "              opt=opt,\n",
    "              buffer=buffer,\n",
    "              cfg=cfg)\n",
    "\n",
    "agent = RLAgent(model=model,\n",
    "                model_key=POLICY,\n",
    "                buffer=buffer,\n",
    "                learner=learner,\n",
    "                key=cfg.agent_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "952d3924",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://localhost:8080/chan/test_jax_rl/runs/3ld2gt8v?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x11b07a590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00043b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/personal_research/jax_learning/jax_learning/rl_utils.py:69\u001b[0m, in \u001b[0;36minteract\u001b[0;34m(env, agent, cfg)\u001b[0m\n\u001b[1;32m     67\u001b[0m render()\n\u001b[1;32m     68\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore(obs, h_state, act, rew, done, info, next_obs, next_h_state)\n\u001b[0;32m---> 69\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_h_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[1;32m     71\u001b[0m ep_return \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rew\n",
      "File \u001b[0;32m~/work/personal_research/jax_learning/jax_learning/agents/agents.py:91\u001b[0m, in \u001b[0;36mLearningAgent.learn\u001b[0;34m(self, next_obs, next_h_state, learn_info)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m, next_obs: np\u001b[38;5;241m.\u001b[39mndarray, next_h_state: np\u001b[38;5;241m.\u001b[39mndarray, learn_info: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_h_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/personal_research/jax_learning/jax_learning/learners/soft_actor_critic.py:353\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, next_obs, next_h_state, learn_info)\u001b[0m\n\u001b[1;32m    345\u001b[0m learn_info\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mACTION_LOG_PROBS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/min_policy_log_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    346\u001b[0m learn_info\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mACTION_LOG_PROBS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/mean_policy_log_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    347\u001b[0m (\n\u001b[1;32m    348\u001b[0m     policy,\n\u001b[1;32m    349\u001b[0m     opt_state,\n\u001b[1;32m    350\u001b[0m     grads,\n\u001b[1;32m    351\u001b[0m     policy_learn_info,\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_key,\n\u001b[0;32m--> 353\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPOLICY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPOLICY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPOLICY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43macts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model[POLICY] \u001b[38;5;241m=\u001b[39m policy\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opt_state[POLICY] \u001b[38;5;241m=\u001b[39m opt_state\n",
      "File \u001b[0;32m~/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/equinox/jit.py:92\u001b[0m, in \u001b[0;36m_JitWrapper.__call__\u001b[0;34m(_JitWrapper__self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(__self, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__self\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/equinox/jit.py:88\u001b[0m, in \u001b[0;36m_JitWrapper._fun_wrapper\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached\u001b[38;5;241m.\u001b[39mlower(dynamic, static)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     dynamic_out, static_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combine(dynamic_out, static_out\u001b[38;5;241m.\u001b[39mvalue)\n",
      "File \u001b[0;32m~/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/equinox/module.py:263\u001b[0m, in \u001b[0;36mModule.tree_unflatten\u001b[0;34m(cls, aux, dynamic_field_values)\u001b[0m\n\u001b[1;32m    256\u001b[0m             dynamic_field_values\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(dynamic_field_values), (\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(dynamic_field_names),\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(static_field_names),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(static_field_values),\n\u001b[1;32m    261\u001b[0m     )\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_unflatten\u001b[39m(\u001b[38;5;28mcls\u001b[39m, aux, dynamic_field_values):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    266\u001b[0m     dynamic_field_names, static_field_names, static_field_values \u001b[38;5;241m=\u001b[39m aux\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "interact(env, agent, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f03ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
