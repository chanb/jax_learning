{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3ba0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import sys\n",
    "import timeit\n",
    "import wandb\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "from jax import grad, jit, vmap\n",
    "from typing import Sequence, Tuple, Dict\n",
    "\n",
    "from jax_learning.agents.rl_agents import RLAgent\n",
    "from jax_learning.buffers.ram_buffers import NextStateNumPyBuffer\n",
    "from jax_learning.buffers.utils import batch_flatten, to_jnp\n",
    "from jax_learning.constants import DISCRETE, CONTINUOUS\n",
    "from jax_learning.rl_utils import interact, evaluate\n",
    "\n",
    "from jax_learning.models import Policy, ActionValue, MLP, StochasticPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a46b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:47: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/chanb/work/personal_research/jax_learning/notebooks/wandb/run-20220626_140924-3o698eyg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/chan/test_jax_rl/runs/3o698eyg\" target=\"_blank\">crisp-darkness-104</a></strong> to <a href=\"http://localhost:8080/chan/test_jax_rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x10ad198d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"test_jax_rl\", group=\"mountaincarcontinuous-reinforce_test\")\n",
    "wandb.define_metric(\"episodic_return\", summary=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68d3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\n",
    "    # Environment setup\n",
    "    \"env\": \"MountainCarContinuous-v0\",\n",
    "    \"seed\": 0,\n",
    "    \"render\": False,\n",
    "    \"clip_action\": True,\n",
    "    \"max_action\": 1.,\n",
    "    \"min_action\": -1.,\n",
    "    \n",
    "    # Experiment progress\n",
    "    \"load_step\": 0,\n",
    "    \"log_interval\": 5000,\n",
    "    \n",
    "    # Learning hyperparameters\n",
    "    \"gamma\": 0.99,\n",
    "    \"max_timesteps\": 1000000,\n",
    "    \"update_frequency\": 4096,\n",
    "    \"lr\": 1e-3,\n",
    "    \"max_grad_norm\": 10.,\n",
    "    \n",
    "    # Model architecture\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_hidden\": 0,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"eval_cfg\": {\n",
    "        \"max_episodes\": 100,\n",
    "        \"seed\": 1,\n",
    "        \"render\": True,\n",
    "        \"clip_action\": True,\n",
    "        \"max_action\": 1.,\n",
    "        \"min_action\": -1.,\n",
    "    }\n",
    "}\n",
    "cfg = Namespace(**cfg_dict)\n",
    "eval_cfg = Namespace(**cfg.eval_cfg)\n",
    "wandb.config = cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733f2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "502213a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(cfg.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ed36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.obs_dim = env.observation_space.shape\n",
    "cfg.act_dim = env.action_space.shape\n",
    "cfg.action_space = CONTINUOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.h_state_dim = (1,)\n",
    "cfg.rew_dim = (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a581441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(env='MountainCarContinuous-v0', seed=0, render=False, clip_action=True, max_action=1.0, min_action=-1.0, load_step=0, log_interval=5000, gamma=0.99, max_timesteps=1000000, update_frequency=4096, lr=0.001, max_grad_norm=10.0, hidden_dim=256, num_hidden=0, eval_cfg={'max_episodes': 100, 'seed': 1, 'render': True, 'clip_action': True, 'max_action': 1.0, 'min_action': -1.0}, obs_dim=(2,), act_dim=(1,), action_space='continuous', h_state_dim=(1,), rew_dim=(1,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c05894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "cfg.buffer_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.env_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.agent_key, cfg.model_key = jrandom.split(jrandom.PRNGKey(cfg.seed), num=2)\n",
    "eval_cfg.env_rng = np.random.RandomState(eval_cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb3d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_learning.distributions import Distribution, Normal\n",
    "\n",
    "\n",
    "class MLPGaussianPolicy(StochasticPolicy):\n",
    "    obs_dim: int\n",
    "    act_dim: int\n",
    "    eps: float\n",
    "    policy: eqx.Module\n",
    "\n",
    "    def __init__(self,\n",
    "                 obs_dim: Sequence[int],\n",
    "                 act_dim: Sequence[int],\n",
    "                 hidden_dim: int,\n",
    "                 num_hidden: int,\n",
    "                 key: jrandom.PRNGKey,\n",
    "                 eps: float=1e-7):\n",
    "        self.obs_dim = int(np.product(obs_dim))\n",
    "        self.act_dim = int(np.product(act_dim))\n",
    "        self.eps = eps\n",
    "        self.policy = MLP(self.obs_dim, self.act_dim * 2, hidden_dim, num_hidden, key)\n",
    "\n",
    "    def deterministic_action(self,\n",
    "                             obs: np.ndarray,\n",
    "                             h_state: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        act_mean, _ = jnp.split(self.policy(obs), 2, axis=-1)\n",
    "        return act_mean, h_state\n",
    "    \n",
    "    def random_action(self,\n",
    "                      obs: np.ndarray,\n",
    "                      h_state: np.ndarray,\n",
    "                      key: jrandom.PRNGKey) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        dist = self.dist(obs, h_state)\n",
    "        act = dist.sample(key)\n",
    "        return act, h_state\n",
    "    \n",
    "    def act_lprob(self,\n",
    "                  obs: np.ndarray,\n",
    "                  h_state: np.ndarray,\n",
    "                  key: jrandom.PRNGKey) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        dist = self.dist(obs, h_state)\n",
    "        act = dist.sample(key)\n",
    "        lprob = dist.lprob(act)\n",
    "        return act, lprob, h_state\n",
    "    \n",
    "    def dist(self,\n",
    "             obs: np.ndarray,\n",
    "             h_state: np.ndarray) -> Distribution:\n",
    "        act_mean, act_raw_std = jnp.split(self.policy(obs), 2, axis=-1)\n",
    "        act_std = jax.nn.softplus(act_raw_std) + self.eps\n",
    "        return Normal(act_mean, act_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66b558a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax_learning.buffers import ReplayBuffer\n",
    "from jax_learning.learners import Learner\n",
    "\n",
    "POLICY = \"policy\"\n",
    "LOSS = \"loss\"\n",
    "MEAN_LOSS = \"mean_loss\"\n",
    "MAX_RETURN = \"max_return\"\n",
    "MIN_RETURN = \"min_return\"\n",
    "class REINFORCE(Learner):\n",
    "    def __init__(self,\n",
    "                 model: Dict[str, eqx.Module],\n",
    "                 opt: Dict[str, optax.GradientTransformation],\n",
    "                 buffer: ReplayBuffer,\n",
    "                 cfg: Namespace):\n",
    "        super().__init__(model, opt, buffer, cfg)\n",
    "        \n",
    "        self._step = cfg.load_step\n",
    "        self._update_frequency = cfg.update_frequency\n",
    "        self._sample_idxes = np.arange(cfg.update_frequency)\n",
    "        self._gamma = cfg.gamma\n",
    "        \n",
    "        def get_lprob(dist, act):\n",
    "            return jnp.sum(dist.lprob(act), keepdims=True)\n",
    "        \n",
    "        def score_function(lprob, ret):\n",
    "            return lprob * ret\n",
    "        \n",
    "        @eqx.filter_grad(has_aux=True)\n",
    "        def reinforce_loss(model: eqx.Module,\n",
    "                           obss: np.ndarray,\n",
    "                           h_states: np.ndarray,\n",
    "                           acts: np.ndarray,\n",
    "                           rets: np.ndarray) -> Tuple[np.ndarray, dict]:\n",
    "            dists = jax.vmap(model.dist)(obss, h_states)\n",
    "            lprobs = jax.vmap(get_lprob)(dists, acts)\n",
    "            scores = jax.vmap(score_function)(lprobs, rets)\n",
    "            loss = -jnp.mean(scores)\n",
    "            return loss, {\n",
    "                LOSS: loss,\n",
    "                MAX_RETURN: jnp.max(rets),\n",
    "                MIN_RETURN: jnp.min(rets),\n",
    "                \"dists\": dists,\n",
    "                \"lprobs\": lprobs,\n",
    "                \"scores\": scores\n",
    "            }\n",
    "        \n",
    "        def step(model: eqx.Module,\n",
    "                 opt: optax.GradientTransformation,\n",
    "                 opt_state: optax.OptState,\n",
    "                 obss: np.ndarray,\n",
    "                 h_states: np.ndarray,\n",
    "                 acts: np.ndarray,\n",
    "                 rets: np.ndarray) -> Tuple[eqx.Module, optax.OptState, jax.tree_util.PyTreeDef, dict]:\n",
    "            grads, learn_info = reinforce_loss(model,\n",
    "                                               obss,\n",
    "                                               h_states,\n",
    "                                               acts,\n",
    "                                               rets)\n",
    "\n",
    "            updates, opt_state = opt.update(grads, opt_state)\n",
    "            model = eqx.apply_updates(model, updates)\n",
    "            return model, opt_state, grads, learn_info\n",
    "        self.step = eqx.filter_jit(step)\n",
    "        \n",
    "    def compute_returns(self, rews, dones):\n",
    "        rets = np.zeros(rews.shape[0] + 1)\n",
    "        for step in reversed(range(len(rews))):\n",
    "            rets[step] = rets[step + 1] * self._gamma * (1 - dones[step]) + rews[step]\n",
    "        return rets[:-1]\n",
    "\n",
    "    def learn(self,\n",
    "              next_obs: np.ndarray,\n",
    "              next_h_state: np.ndarray,\n",
    "              learn_info: dict):\n",
    "        self._step += 1\n",
    "        \n",
    "        if self._step % self._update_frequency != 0:\n",
    "            return\n",
    "\n",
    "        obss, h_states, acts, rews, dones, _, _, _ = self.buffer.sample(batch_size=self._update_frequency,\n",
    "                                                                        idxes=self._sample_idxes)\n",
    "\n",
    "        rets = self.compute_returns(rews, dones)\n",
    "        (obss, h_states, acts, rets) = to_jnp(*batch_flatten(obss,\n",
    "                                                             h_states,\n",
    "                                                             acts,\n",
    "                                                             rets))\n",
    "        model, opt_state, grads, curr_learn_info = self.step(model=self.model[POLICY],\n",
    "                                                             opt=self.opt[POLICY],\n",
    "                                                             opt_state=self.opt_state[POLICY],\n",
    "                                                             obss=obss,\n",
    "                                                             h_states=h_states,\n",
    "                                                             acts=acts,\n",
    "                                                             rets=rets)\n",
    "#         print(curr_learn_info[\"lprobs\"].shape)\n",
    "#         assert 0\n",
    "#         print(curr_learn_info[\"lprobs\"])\n",
    "#         print(curr_learn_info[\"dists\"].mean, curr_learn_info[\"dists\"].std)\n",
    "#         print(curr_learn_info[\"scores\"])\n",
    "#         print(model.policy.weights[-1].weight)\n",
    "#         print(model.policy.biases[-1])\n",
    "#         print(grads.policy.weights[-1].weight)\n",
    "#         print(grads.policy.biases[-1])\n",
    "#         print(\"=\" * 100)\n",
    "#         assert self._step <= 6000\n",
    "\n",
    "        self._model[POLICY] = model\n",
    "        self._opt_state[POLICY] = opt_state\n",
    "\n",
    "        learn_info[MEAN_LOSS] = curr_learn_info[LOSS].item()\n",
    "        self.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e72cdf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "buffer = NextStateNumPyBuffer(\n",
    "    buffer_size=cfg.update_frequency,\n",
    "    obs_dim=cfg.obs_dim,\n",
    "    h_state_dim=cfg.h_state_dim,\n",
    "    act_dim=(1,) if cfg.action_space == DISCRETE else cfg.act_dim,\n",
    "    rew_dim=cfg.rew_dim,\n",
    "    rng=cfg.buffer_rng,\n",
    ")\n",
    "\n",
    "model = {\n",
    "    POLICY: MLPGaussianPolicy(obs_dim=cfg.obs_dim,\n",
    "                              act_dim=cfg.act_dim,\n",
    "                              hidden_dim=cfg.hidden_dim,\n",
    "                              num_hidden=cfg.num_hidden,\n",
    "                              key=cfg.model_key,\n",
    "                              eps=1e-4)\n",
    "}\n",
    "\n",
    "opt = {\n",
    "    POLICY: optax.chain(optax.clip_by_global_norm(cfg.max_grad_norm),  # Clip by the gradient by the global norm\n",
    "                        optax.scale_by_adam(),  # Use the updates from adam\n",
    "                        optax.scale(-cfg.lr)) # Gradient descent\n",
    "}\n",
    "\n",
    "\n",
    "learner = REINFORCE(model=model,\n",
    "                    opt=opt,\n",
    "                    buffer=buffer,\n",
    "                    cfg=cfg)\n",
    "\n",
    "agent = RLAgent(model=model,\n",
    "                model_key=POLICY,\n",
    "                buffer=buffer,\n",
    "                learner=learner,\n",
    "                key=cfg.agent_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "952d3924",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://localhost:8080/chan/test_jax_rl/runs/3o698eyg?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x1205125f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00043b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interact(env, agent, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32851f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env, agent, eval_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
