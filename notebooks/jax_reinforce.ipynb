{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3ba0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import sys\n",
    "import timeit\n",
    "import wandb\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "from jax import grad, jit, vmap\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "from jax_learning.agents.rl_agents import RLAgent\n",
    "from jax_learning.buffers.ram_buffers import NextStateNumPyBuffer\n",
    "from jax_learning.buffers.utils import batch_flatten, to_jnp\n",
    "from jax_learning.constants import DISCRETE, CONTINUOUS\n",
    "from jax_learning.rl_utils import interact, evaluate\n",
    "\n",
    "from jax_learning.models import Policy, ActionValue, MLP, StochasticPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a46b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:47: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.19 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/chanb/work/personal_research/jax_learning/notebooks/wandb/run-20220625_161154-1zu9uab6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/chan/test_jax_rl/runs/1zu9uab6\" target=\"_blank\">bright-gorge-77</a></strong> to <a href=\"http://localhost:8080/chan/test_jax_rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x10f16d1e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"test_jax_rl\", group=\"reacher-reinforce_test\")\n",
    "wandb.define_metric(\"episodic_return\", summary=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68d3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\n",
    "    # Environment setup\n",
    "    \"env\": \"Reacher-v2\",\n",
    "    \"seed\": 0,\n",
    "    \"render\": False,\n",
    "    \n",
    "    # Experiment progress\n",
    "    \"load_step\": 0,\n",
    "    \"log_interval\": 5000,\n",
    "    \n",
    "    # Learning hyperparameters\n",
    "    \"gamma\": 0.99,\n",
    "    \"max_timesteps\": 1000000,\n",
    "    \"update_frequency\": 1024,\n",
    "    \"lr\": 3e-4,\n",
    "    \"max_grad_norm\": 10.,\n",
    "    \n",
    "    # Model architecture\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_hidden\": 2,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"eval_cfg\": {\n",
    "        \"max_episodes\": 100,\n",
    "        \"seed\": 1,\n",
    "        \"render\": True,\n",
    "    }\n",
    "}\n",
    "cfg = Namespace(**cfg_dict)\n",
    "eval_cfg = Namespace(**cfg.eval_cfg)\n",
    "wandb.config = cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733f2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "502213a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/envs/registration.py:564: UserWarning: \u001b[33mWARN: The environment Reacher-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:46: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "objc[24789]: Class GLFWWindowDelegate is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x11f84b7b0) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x11dee2700). One of the two will be used. Which one is undefined.\n",
      "objc[24789]: Class GLFWApplicationDelegate is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x11f84b788) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x11dee2778). One of the two will be used. Which one is undefined.\n",
      "objc[24789]: Class GLFWContentView is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x11f84b800) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x11dee27a0). One of the two will be used. Which one is undefined.\n",
      "objc[24789]: Class GLFWWindow is implemented in both /usr/local/Cellar/glfw/3.3.7/lib/libglfw.3.3.dylib (0x11f84b878) and /Users/chanb/.mujoco/mujoco210/bin/libglfw.3.dylib (0x11dee2818). One of the two will be used. Which one is undefined.\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:69: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:73: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/chanb/work/personal_research/jax_learning/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(cfg.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ed36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.obs_dim = env.observation_space.shape\n",
    "cfg.act_dim = env.action_space.shape\n",
    "cfg.action_space = CONTINUOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.h_state_dim = (1,)\n",
    "cfg.rew_dim = (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a581441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(env='Reacher-v2', seed=0, render=False, load_step=0, log_interval=5000, gamma=0.99, max_timesteps=1000000, update_frequency=1024, lr=0.0003, max_grad_norm=10.0, hidden_dim=256, num_hidden=2, eval_cfg={'max_episodes': 100, 'seed': 1, 'render': True}, obs_dim=(11,), act_dim=(2,), action_space='continuous', h_state_dim=(1,), rew_dim=(1,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c05894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "cfg.buffer_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.env_rng = np.random.RandomState(cfg.seed)\n",
    "cfg.agent_key, cfg.model_key = jrandom.split(jrandom.PRNGKey(cfg.seed), num=2)\n",
    "eval_cfg.env_rng = np.random.RandomState(eval_cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb3d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_learning.distributions import Distribution, Normal\n",
    "\n",
    "\n",
    "class MLPGaussianPolicy(StochasticPolicy):\n",
    "    obs_dim: int\n",
    "    act_dim: int\n",
    "    eps: float\n",
    "    policy: eqx.Module\n",
    "\n",
    "    def __init__(self,\n",
    "                 obs_dim: Sequence[int],\n",
    "                 act_dim: Sequence[int],\n",
    "                 hidden_dim: int,\n",
    "                 num_hidden: int,\n",
    "                 key: jrandom.PRNGKey,\n",
    "                 eps: float=1e-7):\n",
    "        self.obs_dim = int(np.product(obs_dim))\n",
    "        self.act_dim = int(np.product(act_dim))\n",
    "        self.eps = eps\n",
    "        self.policy = MLP(self.obs_dim, self.act_dim * 2, hidden_dim, num_hidden, key)\n",
    "\n",
    "    def deterministic_action(self,\n",
    "                             obs: np.ndarray,\n",
    "                             h_state: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        act_mean, _ = jnp.split(self.policy(obs), 2, axis=-1)\n",
    "        return act_mean, h_state\n",
    "    \n",
    "    def random_action(self,\n",
    "                      obs: np.ndarray,\n",
    "                      h_state: np.ndarray,\n",
    "                      key: jrandom.PRNGKey) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        dist = self.dist(obs, h_state)\n",
    "        act = dist.sample(key)\n",
    "        return act, h_state\n",
    "    \n",
    "    def act_lprob(self,\n",
    "                  obs: np.ndarray,\n",
    "                  h_state: np.ndarray,\n",
    "                  key: jrandom.PRNGKey) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        dist = self.dist(obs, h_state)\n",
    "        act = dist.sample(key)\n",
    "        lprob = dist.lprob(act)\n",
    "        return act, lprob, h_state\n",
    "    \n",
    "    def dist(self,\n",
    "             obs: np.ndarray,\n",
    "             h_state: np.ndarray) -> Distribution:\n",
    "        act_mean, act_raw_std = jnp.split(self.policy(obs), 2, axis=-1)\n",
    "        act_std = jax.nn.softplus(act_raw_std) + self.eps\n",
    "        return Normal(act_mean, act_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66b558a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_learning.buffers import ReplayBuffer\n",
    "from jax_learning.learners import Learner\n",
    "\n",
    "POLICY = \"policy\"\n",
    "LOSS = \"loss\"\n",
    "MAX_RETURN = \"max_return\"\n",
    "MIN_RETURN = \"min_return\"\n",
    "class REINFORCE(Learner):\n",
    "    def __init__(self,\n",
    "                 model: eqx.Module,\n",
    "                 opt: optax.GradientTransformation,\n",
    "                 buffer: ReplayBuffer,\n",
    "                 cfg: Namespace):\n",
    "        super().__init__(model, opt, buffer, cfg)\n",
    "        \n",
    "        self._step = cfg.load_step\n",
    "        self._update_frequency = cfg.update_frequency\n",
    "        self._sample_idxes = np.arange(cfg.update_frequency)\n",
    "        self._gamma = cfg.gamma\n",
    "        \n",
    "        def get_lprob(dist, act):\n",
    "            return jnp.sum(dist.lprob(act))\n",
    "        \n",
    "        def score_function(lprob, ret):\n",
    "            return lprob * ret\n",
    "        \n",
    "        @eqx.filter_grad(has_aux=True)\n",
    "        def reinforce_loss(model: eqx.Module,\n",
    "                           obss: np.ndarray,\n",
    "                           h_states: np.ndarray,\n",
    "                           acts: np.ndarray,\n",
    "                           rets: np.ndarray) -> Tuple[np.ndarray, dict]:\n",
    "            dists = jax.vmap(model.dist)(obss, h_states)\n",
    "            lprobs = jax.vmap(get_lprob)(dists, acts)\n",
    "            \n",
    "            score = jax.vmap(score_function)(lprobs, rets)\n",
    "            loss = -jnp.mean(score)\n",
    "            return loss, {\n",
    "                LOSS: loss,\n",
    "                MAX_RETURN: jnp.max(rets),\n",
    "                MIN_RETURN: jnp.min(rets),\n",
    "            }\n",
    "        \n",
    "        def step(model: eqx.Module,\n",
    "                 opt: optax.GradientTransformation,\n",
    "                 opt_state: optax.OptState,\n",
    "                 obss: np.ndarray,\n",
    "                 h_states: np.ndarray,\n",
    "                 acts: np.ndarray,\n",
    "                 rets: np.ndarray) -> Tuple[eqx.Module, optax.OptState, jax.tree_util.PyTreeDef, dict]:\n",
    "            grads, learn_info = reinforce_loss(model,\n",
    "                                               obss,\n",
    "                                               h_states,\n",
    "                                               acts,\n",
    "                                               rets)\n",
    "\n",
    "            updates, opt_state = opt.update(grads, opt_state)\n",
    "            model = eqx.apply_updates(model, updates)\n",
    "            return model, opt_state, grads, learn_info\n",
    "        self.step = eqx.filter_jit(step)\n",
    "        \n",
    "    def compute_returns(self, rews, dones):\n",
    "        rets = np.zeros(rews.shape[0] + 1)\n",
    "        for step in reversed(range(len(rews))):\n",
    "            rets[step] = rets[step + 1] * self._gamma * (1 - dones[step]) + rews[step]\n",
    "        return rets[:-1]\n",
    "\n",
    "    def learn(self,\n",
    "              next_obs: np.ndarray,\n",
    "              next_h_state: np.ndarray,\n",
    "              learn_info: dict):\n",
    "        self._step += 1\n",
    "        \n",
    "        if self._step % self._update_frequency != 0:\n",
    "            return\n",
    "\n",
    "        obss, h_states, acts, rews, dones, _, _, _ = self.buffer.sample(batch_size=self._update_frequency,\n",
    "                                                                        idxes=self._sample_idxes)\n",
    "\n",
    "        rets = self.compute_returns(rews, dones)\n",
    "        (obss, h_states, acts, rets) = to_jnp(*batch_flatten(obss,\n",
    "                                                             h_states,\n",
    "                                                             acts,\n",
    "                                                             rets))\n",
    "        model, opt_state, grads, curr_learn_info = self.step(model=self.model[POLICY],\n",
    "                                                             opt=self.opt[POLICY],\n",
    "                                                             opt_state=self.opt_state[POLICY],\n",
    "                                                             obss=obss,\n",
    "                                                             h_states=h_states,\n",
    "                                                             acts=acts,\n",
    "                                                             rets=rets)\n",
    "\n",
    "        self._model[POLICY] = model\n",
    "        self._opt_state[POLICY] = opt_state\n",
    "\n",
    "        learn_info[MEAN_LOSS] = curr_learn_info[LOSS].item()\n",
    "        self.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e72cdf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "buffer = NextStateNumPyBuffer(\n",
    "    buffer_size=cfg.update_frequency,\n",
    "    obs_dim=cfg.obs_dim,\n",
    "    h_state_dim=cfg.h_state_dim,\n",
    "    act_dim=(1,) if cfg.action_space == DISCRETE else cfg.act_dim,\n",
    "    rew_dim=cfg.rew_dim,\n",
    "    rng=cfg.buffer_rng,\n",
    ")\n",
    "\n",
    "model = {\n",
    "    POLICY: MLPGaussianPolicy(obs_dim=cfg.obs_dim,\n",
    "                              act_dim=cfg.act_dim,\n",
    "                              hidden_dim=cfg.hidden_dim,\n",
    "                              num_hidden=cfg.num_hidden,\n",
    "                              key=cfg.model_key)\n",
    "}\n",
    "\n",
    "opt = {\n",
    "    POLICY: optax.chain(optax.clip_by_global_norm(cfg.max_grad_norm),  # Clip by the gradient by the global norm\n",
    "                        optax.scale_by_adam(),  # Use the updates from adam\n",
    "                        optax.scale(-1.0)) # Gradient descent\n",
    "}\n",
    "\n",
    "\n",
    "learner = REINFORCE(model=model,\n",
    "                    opt=opt,\n",
    "                    buffer=buffer,\n",
    "                    cfg=cfg)\n",
    "\n",
    "agent = RLAgent(model=model[POLICY],\n",
    "                buffer=buffer,\n",
    "                learner=learner,\n",
    "                key=cfg.agent_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "952d3924",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://localhost:8080/chan/test_jax_rl/runs/1zu9uab6?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x127450250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d00043b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MEAN_LOSS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/personal_research/jax_learning/jax_learning/rl_utils.py:28\u001b[0m, in \u001b[0;36minteract\u001b[0;34m(env, agent, cfg)\u001b[0m\n\u001b[1;32m     26\u001b[0m render()\n\u001b[1;32m     27\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore(obs, h_state, act, rew, done, info, next_obs, next_h_state)\n\u001b[0;32m---> 28\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_h_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[1;32m     30\u001b[0m ep_return \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rew\n",
      "File \u001b[0;32m~/work/personal_research/jax_learning/jax_learning/agents/agents.py:84\u001b[0m, in \u001b[0;36mLearningAgent.learn\u001b[0;34m(self, next_obs, next_h_state, learn_info)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     81\u001b[0m           next_obs: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m     82\u001b[0m           next_h_state: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m     83\u001b[0m           learn_info: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_h_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36mREINFORCE.learn\u001b[0;34m(self, next_obs, next_h_state, learn_info)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model[POLICY] \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opt_state[POLICY] \u001b[38;5;241m=\u001b[39m opt_state\n\u001b[0;32m---> 96\u001b[0m learn_info[\u001b[43mMEAN_LOSS\u001b[49m] \u001b[38;5;241m=\u001b[39m curr_learn_info[LOSS]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MEAN_LOSS' is not defined"
     ]
    }
   ],
   "source": [
    "interact(env, agent, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32851f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env, agent, eval_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7794fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.roll(buffer.next_observations, 1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116971b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((buffer.observations - np.roll(buffer.next_observations, 1, axis=0), buffer.dones), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f03ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.sample_with_next_obs(3, buffer.next_observations[19], buffer.hidden_states[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
